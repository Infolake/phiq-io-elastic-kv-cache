{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b176897d",
   "metadata": {
    "id": "b176897d"
   },
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Infolake/phiq-io-elastic-kv-cache/master/notebooks/content/logo-phi-q-icon-256.png\" alt=\"PHIQ.IO Logo\" width=\"100\"/>\n",
    "\n",
    "## ΦQ™ PHIQ.IO — Elastic KV Cache (Golden Ticket Edition)\n",
    "Self-contained, production-grade LLM microbenchmark\n",
    "Paired baseline • CUDA Graphs • Vectorized `float4` loads • Inference cycle timing • Roofline metrics\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### Notes\n",
    "- This notebook **embeds the CUDA source** and compiles it locally (no repo clone required).\n",
    "- It runs reliably on Colab GPUs (T4/L4/A100). For other GPUs, adjust `-gencode` flags in the compile cell.\n",
    "- The **GGUF section is optional** and off by default—enable when you want to showcase inference on hype models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10504873",
   "metadata": {
    "id": "10504873"
   },
   "source": [
    "\n",
    "## 1) Runtime & High-RAM\n",
    "\n",
    "- In Colab: **Runtime → Change runtime type → GPU** (T4/L4/A100 are fine).\n",
    "- **High-RAM**: turn it **ON** if you plan to download models ≥ ~7B or do large experiments.\n",
    "  High-RAM increases **host RAM**, which helps with big downloads & preprocessing (not GPU VRAM).\n",
    "- After changing runtime, rerun from the top.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa922400",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fa922400",
    "outputId": "d0317022-8179-4049-a68a-7bce613cb8bd"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 2) GPU sanity check\n",
    "!nvidia-smi || true\n",
    "!nvcc --version || true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3518cd6d",
   "metadata": {
    "id": "3518cd6d"
   },
   "source": [
    "\n",
    "## 3) Hugging Face Login (secure)\n",
    "\n",
    "Use the interactive prompt. **Do NOT commit your personal token** into a public repo.\n",
    "\n",
    "- The token line below is **commented** on purpose.\n",
    "- GGUF section later can use this if you enable it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e0bc2",
   "metadata": {
    "id": "942e0bc2"
   },
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# login()  # ← Recommended (interactive prompt)\n",
    "# WARNING: do not hardcode tokens in public notebooks:\n",
    "# login(token=\"hf_your_personal_access_token_here\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dafd2a",
   "metadata": {
    "id": "97dafd2a"
   },
   "source": [
    "\n",
    "## 4) Controls\n",
    "\n",
    "Toggle optional tracks. Defaults keep the run fast and robust for demos/judging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416bf228",
   "metadata": {
    "id": "416bf228"
   },
   "outputs": [],
   "source": [
    "\n",
    "ENABLE_GGUF = False     # Set True to include GGUF + llama.cpp timing (optional, heavier)\n",
    "GGUF_REPO   = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n",
    "GGUF_FILE   = \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
    "\n",
    "# Transformers mini-baseline (tiny model, fast)\n",
    "ENABLE_TRANSFORMERS_MINI = True\n",
    "TRANSFORMERS_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # small, runs on T4 with fp16\n",
    "DECODE_TOKENS = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba108ff",
   "metadata": {
    "id": "cba108ff"
   },
   "source": [
    "\n",
    "## 5) Write CUDA source (embedded Golden Ticket kernel)\n",
    "\n",
    "This is the production microbenchmark with race-free double-buffer + ping-pong CUDA Graphs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d4d1ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71d4d1ad",
    "outputId": "ad592d09-11b6-4ff7-d0e8-accf2b44dac1"
   },
   "outputs": [],
   "source": [
    "%%writefile elastic_kv_cli.cu\n",
    "// ============================================================================\n",
    "//  ΦQ™ PHIQ.IO Elastic KV Core – Golden Ticket Edition – GOE Nucleus\n",
    "//  Author: Dr. Guilherme de Camargo\n",
    "//  Organization: PHIQ.IO Quantum Technologies (ΦQ™)\n",
    "//  Contact: https://phiq.io | support@phiq.io\n",
    "//  © 2025 PHIQ.IO Quantum Technologies. All rights reserved.\n",
    "//\n",
    "//  Description: Production-grade elastic key-value cache for LLM inference\n",
    "//               Paired Baseline, CUDA Graphs, Vectorized float4 loads,\n",
    "//               Roofline scoring, Statistical CV, Inference Cycle timing\n",
    "//  Target: High-performance CUDA, Multi-GPU (Pascal SM 6.1 through Hopper SM 9.0)\n",
    "//  License: See LICENSE file for terms of use\n",
    "//\n",
    "//  Camargo Constant: Δ = φ + π = 4.759627\n",
    "// ============================================================================\n",
    "\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda_fp16.h>\n",
    "#include <cstdio>\n",
    "#include <cstdlib>\n",
    "#include <chrono>\n",
    "#include <cmath>\n",
    "#include <cstring>\n",
    "#include <vector>\n",
    "#include <algorithm>\n",
    "\n",
    "#define CUDA_CHECK(call) do { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        printf(\"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(err), __FILE__, __LINE__); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "} while(0)\n",
    "\n",
    "// GTX 1070 / Pascal-friendly defaults\n",
    "#define OPTIMAL_BLOCK_SIZE 256\n",
    "#define VECTOR_WIDTH 4\n",
    "#define THEORETICAL_BW_GBS 256.0f // GTX 1070\n",
    "\n",
    "struct ElasticKVConfig {\n",
    "    // Workload\n",
    "    int seq_len = 1024;\n",
    "    int heads = 16;\n",
    "    int head_dim = 64;\n",
    "    int compression_ratio = 2;\n",
    "\n",
    "    // Measurement\n",
    "    int warmup_iterations = 20;\n",
    "    int test_iterations = 200;\n",
    "    int inner_loops = 64;          // repeats per timed sample (reduces jitter)\n",
    "    int truncate_percent = 5;      // trimmed mean percent (default 5% for Colab/L4 stability)\n",
    "\n",
    "    // Modes\n",
    "    bool enable_cuda_graphs = true;\n",
    "    bool enable_json_output = true;\n",
    "    bool paired_baseline = false;  // run baseline (compress=1) and elastic in one invocation\n",
    "\n",
    "    // Inference cycle (sequential decode steps)\n",
    "    bool measure_inference_cycle = false;\n",
    "    int decode_tokens = 64;        // number of sequential attention passes to simulate decode\n",
    "\n",
    "    // Branding\n",
    "    const char* brand = \"PHIQ IO GOE Nucleus\";\n",
    "    const char* mode = \"kv\";\n",
    "};\n",
    "\n",
    "struct BenchmarkResults {\n",
    "    // Microbench (single attention pass averaged)\n",
    "    float attention_time_ms = 0.0f;\n",
    "    float attention_time_std = 0.0f;\n",
    "    float tokens_per_sec = 0.0f;\n",
    "\n",
    "    // Bandwidth and roofline\n",
    "    float memory_bandwidth_gbs = 0.0f;\n",
    "    float memory_efficiency_percent = 0.0f;\n",
    "    float roofline_score = 0.0f;\n",
    "\n",
    "    // Baseline comparison\n",
    "    float baseline_tokens_per_sec = 0.0f;\n",
    "    float speedup_vs_baseline = 0.0f;\n",
    "};\n",
    "\n",
    "struct InferenceCycleResults {\n",
    "    bool measured = false;\n",
    "    int decode_tokens = 0;\n",
    "    float baseline_total_ms = 0.0f;\n",
    "    float elastic_total_ms = 0.0f;\n",
    "    float baseline_tokens_per_sec = 0.0f;\n",
    "    float elastic_tokens_per_sec = 0.0f;\n",
    "    float speedup_vs_baseline = 0.0f;\n",
    "};\n",
    "\n",
    "// ----------------------------------------------------------------------------\n",
    "// Kernel (Pascal-optimized path with float4 vector loads + double-buffer)\n",
    "// Double-buffer eliminates race condition: always read from O_prev, write to O_out\n",
    "// This ensures audit-ready reproducibility and stable CV measurements\n",
    "// ----------------------------------------------------------------------------\n",
    "__global__ void __launch_bounds__(OPTIMAL_BLOCK_SIZE)\n",
    "elastic_attention_pascal_optimized(\n",
    "    const float4* __restrict__ Q,\n",
    "    const float4* __restrict__ K,\n",
    "    const float4* __restrict__ V,\n",
    "    const float4* __restrict__ O_prev,  // Read buffer (previous iteration)\n",
    "    float4* __restrict__ O_out,         // Write buffer (current iteration)\n",
    "    int seq_len, int num_heads, int head_dim_vec,\n",
    "    int compression_factor,\n",
    "    float scale_factor\n",
    ") {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int total_vec = seq_len * num_heads * head_dim_vec;\n",
    "    if (tid >= total_vec) return;\n",
    "\n",
    "    int per_head = seq_len * head_dim_vec;\n",
    "    int head_idx = tid / per_head;\n",
    "    int rem = tid % per_head;\n",
    "    int seq_idx = rem / head_dim_vec;\n",
    "    int dim_idx_vec = rem % head_dim_vec;\n",
    "\n",
    "    float4 q = Q[tid];\n",
    "    float4 k = K[tid];\n",
    "    float4 v = V[tid];\n",
    "\n",
    "    if ((seq_idx % compression_factor) == 0) {\n",
    "        float dot = (q.x*k.x + q.y*k.y + q.z*k.z + q.w*k.w) * scale_factor;\n",
    "        float s = expf(dot); // simplified softmax-like weight\n",
    "        O_out[tid] = make_float4(s*v.x, s*v.y, s*v.z, s*v.w);\n",
    "    } else {\n",
    "        int anchor = (seq_idx / compression_factor) * compression_factor;\n",
    "        int anchor_tid = head_idx * per_head + anchor * head_dim_vec + dim_idx_vec;\n",
    "        float4 cached = O_prev[anchor_tid];  // Always read from previous buffer\n",
    "        O_out[tid] = make_float4(0.95f*cached.x, 0.95f*cached.y, 0.95f*cached.z, 0.95f*cached.w);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void __launch_bounds__(OPTIMAL_BLOCK_SIZE)\n",
    "memory_bandwidth_stream(\n",
    "    const float4* __restrict__ input,\n",
    "    float4* __restrict__ output,\n",
    "    int size_vec\n",
    ") {\n",
    "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (tid >= size_vec) return;\n",
    "    float4 d = input[tid];\n",
    "    output[tid] = make_float4(d.x + 1.0f, d.y + 1.0f, d.z + 1.0f, d.w + 1.0f);\n",
    "}\n",
    "\n",
    "// ----------------------------------------------------------------------------\n",
    "// Benchmark harness\n",
    "// ----------------------------------------------------------------------------\n",
    "class ElasticKVBenchmark {\n",
    "public:\n",
    "    ElasticKVBenchmark(const ElasticKVConfig& cfg) : config(cfg) {\n",
    "        if (config.head_dim % VECTOR_WIDTH != 0) {\n",
    "            printf(\"Error: head_dim must be divisible by %d (float4 vectorization).\\n\", VECTOR_WIDTH);\n",
    "            exit(2);\n",
    "        }\n",
    "        // Guard rails for audit-ready execution\n",
    "        if (config.seq_len <= 0 || config.heads <= 0 || config.head_dim <= 0) {\n",
    "            printf(\"Error: Invalid configuration (seq_len=%d, heads=%d, head_dim=%d)\\n\",\n",
    "                   config.seq_len, config.heads, config.head_dim);\n",
    "            exit(2);\n",
    "        }\n",
    "        initialize();\n",
    "        if (config.enable_cuda_graphs) {\n",
    "            buildPingPongGraphs();\n",
    "            // Upload graphs to device to reduce first-launch jitter (audit-ready)\n",
    "            CUDA_CHECK(cudaGraphUpload(exec_baseline_p2o, 0));\n",
    "            CUDA_CHECK(cudaGraphUpload(exec_baseline_o2p, 0));\n",
    "            CUDA_CHECK(cudaGraphUpload(exec_elastic_p2o, 0));\n",
    "            CUDA_CHECK(cudaGraphUpload(exec_elastic_o2p, 0));\n",
    "        }\n",
    "    }\n",
    "\n",
    "    ~ElasticKVBenchmark() {\n",
    "        // Cleanup ping-pong graphs\n",
    "        if (exec_elastic_o2p) cudaGraphExecDestroy(exec_elastic_o2p);\n",
    "        if (exec_elastic_p2o) cudaGraphExecDestroy(exec_elastic_p2o);\n",
    "        if (graph_elastic_o2p) cudaGraphDestroy(graph_elastic_o2p);\n",
    "        if (graph_elastic_p2o) cudaGraphDestroy(graph_elastic_p2o);\n",
    "        if (exec_baseline_o2p) cudaGraphExecDestroy(exec_baseline_o2p);\n",
    "        if (exec_baseline_p2o) cudaGraphExecDestroy(exec_baseline_p2o);\n",
    "        if (graph_baseline_o2p) cudaGraphDestroy(graph_baseline_o2p);\n",
    "        if (graph_baseline_p2o) cudaGraphDestroy(graph_baseline_p2o);\n",
    "\n",
    "        cudaFree(d_Q); cudaFree(d_K); cudaFree(d_V);\n",
    "        cudaFree(d_O_prev); cudaFree(d_O_out);\n",
    "        cudaFree(d_mem_in); cudaFree(d_mem_out);\n",
    "    }\n",
    "\n",
    "    BenchmarkResults runMicrobench() {\n",
    "        BenchmarkResults r{};\n",
    "\n",
    "        // Warm-up elastic with ping-pong\n",
    "        for (int i = 0; i < config.warmup_iterations; ++i) {\n",
    "            (void) runPass(exec_elastic_p2o, exec_elastic_o2p, true);\n",
    "        }\n",
    "        CUDA_CHECK(cudaDeviceSynchronize());\n",
    "\n",
    "        // Timed elastic samples with inner loops and ping-pong\n",
    "        std::vector<float> samples; samples.reserve(config.test_iterations);\n",
    "        for (int i = 0; i < config.test_iterations; ++i) {\n",
    "            float ms = runPass(exec_elastic_p2o, exec_elastic_o2p, true);\n",
    "            samples.push_back(ms);\n",
    "        }\n",
    "\n",
    "        // Optional trimmed mean (audit-ready: removes thermal outliers)\n",
    "        auto stats = samples;\n",
    "        if (config.truncate_percent > 0 && stats.size() > 20) {\n",
    "            std::sort(stats.begin(), stats.end());\n",
    "            int cut = (int)(stats.size() * (config.truncate_percent / 100.0f));\n",
    "            cut = std::min(cut, (int)stats.size()/10);\n",
    "            stats = std::vector<float>(stats.begin()+cut, stats.end()-cut);\n",
    "        }\n",
    "        double s=0, s2=0; for (float v: stats) { s+=v; s2+=v*v; }\n",
    "        double mean = s / stats.size();\n",
    "        double var  = s2 / stats.size() - mean*mean;\n",
    "        double stdv = var > 0 ? std::sqrt(var) : 0;\n",
    "\n",
    "        r.attention_time_ms  = (float)mean;\n",
    "        r.attention_time_std = (float)stdv;\n",
    "        r.tokens_per_sec     = 1000.0f / r.attention_time_ms;\n",
    "\n",
    "        // Baseline tokens/s (compress=1) using identical ping-pong pipeline\n",
    "        r.baseline_tokens_per_sec = measureBaselineTokensPerSec();\n",
    "\n",
    "        // BW and roofline\n",
    "        r.memory_bandwidth_gbs = measureBandwidthGBs();\n",
    "        r.memory_efficiency_percent = (r.memory_bandwidth_gbs / THEORETICAL_BW_GBS) * 100.0f;\n",
    "\n",
    "        float bw_eff = r.memory_bandwidth_gbs / THEORETICAL_BW_GBS;\n",
    "        float comp_eff = (r.baseline_tokens_per_sec > 0.f)\n",
    "            ? (r.tokens_per_sec / r.baseline_tokens_per_sec) : 0.f;\n",
    "\n",
    "        r.roofline_score = std::min(1.0f, 0.5f*bw_eff + 0.5f*comp_eff);\n",
    "        r.speedup_vs_baseline = comp_eff;\n",
    "\n",
    "        return r;\n",
    "    }\n",
    "\n",
    "    InferenceCycleResults runInferenceCycle() {\n",
    "        InferenceCycleResults ir{};\n",
    "        if (!config.measure_inference_cycle) return ir;\n",
    "        ir.measured = true;\n",
    "        ir.decode_tokens = config.decode_tokens;\n",
    "\n",
    "        // Baseline sequence (compress=1) with ping-pong\n",
    "        float base_ms = timeSequential(exec_baseline_p2o, exec_baseline_o2p, config.decode_tokens);\n",
    "        // Elastic sequence (compress=config.compression_ratio) with ping-pong\n",
    "        float elas_ms = timeSequential(exec_elastic_p2o, exec_elastic_o2p, config.decode_tokens);\n",
    "\n",
    "        ir.baseline_total_ms = base_ms;\n",
    "        ir.elastic_total_ms  = elas_ms;\n",
    "\n",
    "        ir.baseline_tokens_per_sec = (base_ms > 0) ? (1000.0f * config.decode_tokens / base_ms) : 0.f;\n",
    "        ir.elastic_tokens_per_sec  = (elas_ms > 0) ? (1000.0f * config.decode_tokens / elas_ms) : 0.f;\n",
    "        ir.speedup_vs_baseline = (ir.baseline_tokens_per_sec > 0)\n",
    "                                 ? (ir.elastic_tokens_per_sec / ir.baseline_tokens_per_sec) : 0.f;\n",
    "        return ir;\n",
    "    }\n",
    "\n",
    "    static void printGPUInfo() {\n",
    "        cudaDeviceProp p; CUDA_CHECK(cudaGetDeviceProperties(&p, 0));\n",
    "        printf(\"GPU: %s (SM %d.%d)\\n\", p.name, p.major, p.minor);\n",
    "        printf(\"Global Memory: %.1f MB\\n\", p.totalGlobalMem / (1024.0 * 1024.0));\n",
    "        printf(\"SMs: %d | Max Threads/Block: %d\\n\", p.multiProcessorCount, p.maxThreadsPerBlock);\n",
    "        printf(\"Theoretical Bandwidth: %.1f GB/s\\n\", THEORETICAL_BW_GBS);\n",
    "    }\n",
    "\n",
    "    static void printUsage(const char* prog) {\n",
    "        printf(\"========================================================================\\n\");\n",
    "        printf(\"  ΦQ™ PHIQ.IO Elastic KV Cache - Golden Ticket Edition\\n\");\n",
    "        printf(\"  GOE Nucleus | Production-Grade LLM Inference Acceleration\\n\");\n",
    "        printf(\"  Author: Dr. Guilherme de Camargo | Camargo Constant: Δ = 4.759627\\n\");\n",
    "        printf(\"========================================================================\\n\\n\");\n",
    "        printf(\"Usage: %s [options]\\n\\n\", prog);\n",
    "        printf(\"Options:\\n\");\n",
    "        printf(\"  --seq=N              Sequence length (default 1024)\\n\");\n",
    "        printf(\"  --dim=D              Head dimension (default 64)\\n\");\n",
    "        printf(\"  --heads=H            Number of heads (default 16)\\n\");\n",
    "        printf(\"  --compress=C         Compression ratio (default 2, min 1)\\n\");\n",
    "        printf(\"  --reps=R             Timed iterations (default 200)\\n\");\n",
    "        printf(\"  --warmup=W           Warmup iterations (default 20)\\n\");\n",
    "        printf(\"  --inner_loops=K      Passes per sample (default 64)\\n\");\n",
    "        printf(\"  --truncate=P         Trimmed mean percent (0..45, default 5)\\n\");\n",
    "        printf(\"  --paired-baseline    Measure baseline (C=1) and elastic in one run\\n\");\n",
    "        printf(\"  --inference          Measure inference cycle (sequential decode)\\n\");\n",
    "        printf(\"  --decode_tokens=T    Number of sequential steps (default 64)\\n\");\n",
    "        printf(\"  --no-graphs          Disable CUDA Graphs\\n\");\n",
    "        printf(\"  --json               JSON output (default true)\\n\");\n",
    "        printf(\"  --no-json            Human-readable output\\n\");\n",
    "        printf(\"  --help               Show this help\\n\");\n",
    "        printf(\"\\nExamples:\\n\");\n",
    "        printf(\"  Basic benchmark:\\n\");\n",
    "        printf(\"    %s --seq=1024 --compress=2 --json\\n\\n\", prog);\n",
    "        printf(\"  Production audit (paired baseline + inference cycle):\\n\");\n",
    "        printf(\"    %s --seq=4096 --heads=32 --dim=128 --compress=4 --reps=120 \\\\\\n\", prog);\n",
    "        printf(\"      --warmup=60 --inner_loops=64 --truncate=5 --json \\\\\\n\");\n",
    "        printf(\"      --paired-baseline --inference --decode_tokens=128\\n\\n\");\n",
    "        printf(\"Contact: https://phiq.io | support@phiq.io\\n\");\n",
    "    }\n",
    "\n",
    "    static void outputJSON(const ElasticKVConfig& c, const BenchmarkResults& r, const InferenceCycleResults& ir) {\n",
    "        cudaDeviceProp p; CUDA_CHECK(cudaGetDeviceProperties(&p, 0));\n",
    "        printf(\"{\\n\");\n",
    "        printf(\"  \\\"benchmark_type\\\": \\\"elastic_kv_golden_ticket_en\\\",\\n\");\n",
    "        printf(\"  \\\"brand\\\": \\\"%s\\\",\\n\", c.brand);\n",
    "        printf(\"  \\\"build\\\": { \\\"cuda_graphs\\\": %s, \\\"inner_loops\\\": %d, \\\"truncate_percent\\\": %d },\\n\",\n",
    "               c.enable_cuda_graphs ? \"true\" : \"false\", c.inner_loops, c.truncate_percent);\n",
    "        printf(\"  \\\"gpu\\\": { \\\"name\\\": \\\"%s\\\", \\\"sm\\\": \\\"%d.%d\\\", \\\"theoretical_bw_gbs\\\": %.1f },\\n\",\n",
    "               p.name, p.major, p.minor, THEORETICAL_BW_GBS);\n",
    "        printf(\"  \\\"configuration\\\": { \\\"seq_len\\\": %d, \\\"heads\\\": %d, \\\"head_dim\\\": %d, \\\"compression\\\": %d, \\\"reps\\\": %d, \\\"warmup\\\": %d },\\n\",\n",
    "               c.seq_len, c.heads, c.head_dim, c.compression_ratio, c.test_iterations, c.warmup_iterations);\n",
    "        printf(\"  \\\"results\\\": {\\n\");\n",
    "        printf(\"    \\\"attention_time_ms\\\": %.6f,\\n\", r.attention_time_ms);\n",
    "        printf(\"    \\\"attention_time_std\\\": %.6f,\\n\", r.attention_time_std);\n",
    "        printf(\"    \\\"coefficient_of_variation\\\": %.6f,\\n\", (r.attention_time_ms>0)?(r.attention_time_std/r.attention_time_ms):0.0f);\n",
    "        printf(\"    \\\"tokens_per_sec\\\": %.3f,\\n\", r.tokens_per_sec);\n",
    "        printf(\"    \\\"baseline_tokens_per_sec\\\": %.3f,\\n\", r.baseline_tokens_per_sec);\n",
    "        printf(\"    \\\"speedup_vs_baseline\\\": %.3f,\\n\", r.speedup_vs_baseline);\n",
    "        printf(\"    \\\"memory_bandwidth_gbs\\\": %.2f,\\n\", r.memory_bandwidth_gbs);\n",
    "        printf(\"    \\\"memory_efficiency_percent\\\": %.1f,\\n\", r.memory_efficiency_percent);\n",
    "        printf(\"    \\\"roofline_score\\\": %.3f\\n\", r.roofline_score);\n",
    "        printf(\"  },\\n\");\n",
    "        if (ir.measured) {\n",
    "            printf(\"  \\\"inference_cycle\\\": {\\n\");\n",
    "            printf(\"    \\\"decode_tokens\\\": %d,\\n\", ir.decode_tokens);\n",
    "            printf(\"    \\\"baseline_total_ms\\\": %.6f,\\n\", ir.baseline_total_ms);\n",
    "            printf(\"    \\\"elastic_total_ms\\\": %.6f,\\n\", ir.elastic_total_ms);\n",
    "            printf(\"    \\\"baseline_tokens_per_sec\\\": %.3f,\\n\", ir.baseline_tokens_per_sec);\n",
    "            printf(\"    \\\"elastic_tokens_per_sec\\\": %.3f,\\n\", ir.elastic_tokens_per_sec);\n",
    "            printf(\"    \\\"speedup_vs_baseline\\\": %.3f\\n\", ir.speedup_vs_baseline);\n",
    "            printf(\"  }\\n\");\n",
    "        } else {\n",
    "            printf(\"  \\\"inference_cycle\\\": null\\n\");\n",
    "        }\n",
    "        printf(\"}\\n\");\n",
    "    }\n",
    "\n",
    "private:\n",
    "    ElasticKVConfig config;\n",
    "    // Device buffers (double-buffer for race-free execution)\n",
    "    float4 *d_Q=nullptr, *d_K=nullptr, *d_V=nullptr;\n",
    "    float4 *d_O_prev=nullptr, *d_O_out=nullptr;  // Ping-pong buffers\n",
    "    float4 *d_mem_in=nullptr, *d_mem_out=nullptr;\n",
    "\n",
    "    // Ping-pong graphs: G₀ (prev→out) and G₁ (out→prev) for baseline and elastic\n",
    "    cudaGraph_t graph_baseline_p2o=nullptr, graph_baseline_o2p=nullptr;\n",
    "    cudaGraph_t graph_elastic_p2o=nullptr, graph_elastic_o2p=nullptr;\n",
    "    cudaGraphExec_t exec_baseline_p2o=nullptr, exec_baseline_o2p=nullptr;\n",
    "    cudaGraphExec_t exec_elastic_p2o=nullptr, exec_elastic_o2p=nullptr;\n",
    "\n",
    "    void initialize() {\n",
    "        int head_dim_vec = config.head_dim / VECTOR_WIDTH;\n",
    "        size_t tensors_vec = (size_t)config.seq_len * config.heads * head_dim_vec;\n",
    "        size_t bytes = tensors_vec * sizeof(float4);\n",
    "\n",
    "        CUDA_CHECK(cudaMalloc(&d_Q, bytes));\n",
    "        CUDA_CHECK(cudaMalloc(&d_K, bytes));\n",
    "        CUDA_CHECK(cudaMalloc(&d_V, bytes));\n",
    "\n",
    "        // Double-buffer allocation for race-free ping-pong execution\n",
    "        CUDA_CHECK(cudaMalloc(&d_O_prev, bytes));\n",
    "        CUDA_CHECK(cudaMalloc(&d_O_out, bytes));\n",
    "        // Initialize O_prev with zeros for reproducibility\n",
    "        CUDA_CHECK(cudaMemset(d_O_prev, 0, bytes));\n",
    "        CUDA_CHECK(cudaMemset(d_O_out, 0, bytes));\n",
    "\n",
    "        // Fill host vectors with synthetic but stable data (LCG for reproducibility)\n",
    "        std::vector<float4> h(tensors_vec);\n",
    "        for (size_t i=0;i<h.size();++i) {\n",
    "            float base = (float)((i*1664525u + 1013904223u) & 0xFFFF) / 65535.0f; // LCG-ish\n",
    "            h[i] = make_float4(base, base*0.5f, -base, 0.25f - base);\n",
    "        }\n",
    "        CUDA_CHECK(cudaMemcpy(d_Q, h.data(), bytes, cudaMemcpyHostToDevice));\n",
    "        CUDA_CHECK(cudaMemcpy(d_K, h.data(), bytes, cudaMemcpyHostToDevice));\n",
    "        CUDA_CHECK(cudaMemcpy(d_V, h.data(), bytes, cudaMemcpyHostToDevice));\n",
    "\n",
    "        // Bandwidth buffers (~64MB in float4)\n",
    "        int sz_vec = 16 * 1024 * 1024;\n",
    "        CUDA_CHECK(cudaMalloc(&d_mem_in,  sz_vec * sizeof(float4)));\n",
    "        CUDA_CHECK(cudaMalloc(&d_mem_out, sz_vec * sizeof(float4)));\n",
    "    }\n",
    "\n",
    "    void buildSingleGraph(int comp_ratio, const float4* O_src, float4* O_dst,\n",
    "                          cudaGraph_t& g, cudaGraphExec_t& e) {\n",
    "        cudaStream_t s; CUDA_CHECK(cudaStreamCreate(&s));\n",
    "        CUDA_CHECK(cudaStreamBeginCapture(s, cudaStreamCaptureModeGlobal));\n",
    "\n",
    "        int head_dim_vec = config.head_dim / VECTOR_WIDTH;\n",
    "        int total_vec = config.seq_len * config.heads * head_dim_vec;\n",
    "        int blocks = (total_vec + OPTIMAL_BLOCK_SIZE - 1) / OPTIMAL_BLOCK_SIZE;\n",
    "        float scale = 1.0f / sqrtf((float)config.head_dim);\n",
    "\n",
    "        // No shared memory needed (removed unused dynamic smem allocation)\n",
    "        elastic_attention_pascal_optimized<<<blocks, OPTIMAL_BLOCK_SIZE, 0, s>>>(\n",
    "            d_Q, d_K, d_V, O_src, O_dst,\n",
    "            config.seq_len, config.heads, head_dim_vec,\n",
    "            comp_ratio, scale);\n",
    "\n",
    "        CUDA_CHECK(cudaStreamEndCapture(s, &g));\n",
    "        CUDA_CHECK(cudaGraphInstantiate(&e, g, nullptr, nullptr, 0));\n",
    "        CUDA_CHECK(cudaStreamDestroy(s));\n",
    "    }\n",
    "\n",
    "    void buildPingPongGraphs() {\n",
    "        // Baseline graphs (compress=1): G₀ (prev→out) and G₁ (out→prev)\n",
    "        buildSingleGraph(1, d_O_prev, d_O_out, graph_baseline_p2o, exec_baseline_p2o);\n",
    "        buildSingleGraph(1, d_O_out, d_O_prev, graph_baseline_o2p, exec_baseline_o2p);\n",
    "\n",
    "        // Elastic graphs (compress=config.compression_ratio): G₀ and G₁\n",
    "        buildSingleGraph(config.compression_ratio, d_O_prev, d_O_out,\n",
    "                         graph_elastic_p2o, exec_elastic_p2o);\n",
    "        buildSingleGraph(config.compression_ratio, d_O_out, d_O_prev,\n",
    "                         graph_elastic_o2p, exec_elastic_o2p);\n",
    "    }\n",
    "\n",
    "    // Time a single mean pass with ping-pong (averaged over inner_loops) using cudaEvents\n",
    "    float runPass(cudaGraphExec_t exec_p2o, cudaGraphExec_t exec_o2p, bool use_graphs) {\n",
    "        cudaEvent_t start, stop; CUDA_CHECK(cudaEventCreate(&start)); CUDA_CHECK(cudaEventCreate(&stop));\n",
    "\n",
    "        int head_dim_vec = config.head_dim / VECTOR_WIDTH;\n",
    "        int total_vec = config.seq_len * config.heads * head_dim_vec;\n",
    "        int blocks = (total_vec + OPTIMAL_BLOCK_SIZE - 1) / OPTIMAL_BLOCK_SIZE;\n",
    "        float scale = 1.0f / sqrtf((float)config.head_dim);\n",
    "\n",
    "        CUDA_CHECK(cudaEventRecord(start, 0));\n",
    "        for (int i = 0; i < config.inner_loops; ++i) {\n",
    "            if (config.enable_cuda_graphs && use_graphs) {\n",
    "                // Ping-pong: alternate between prev→out and out→prev\n",
    "                CUDA_CHECK(cudaGraphLaunch(exec_p2o, 0));\n",
    "                CUDA_CHECK(cudaGraphLaunch(exec_o2p, 0));\n",
    "            } else {\n",
    "                // Fallback: manual launch with ping-pong (no shared mem)\n",
    "                elastic_attention_pascal_optimized<<<blocks, OPTIMAL_BLOCK_SIZE, 0>>>(\n",
    "                    d_Q, d_K, d_V, d_O_prev, d_O_out,\n",
    "                    config.seq_len, config.heads, head_dim_vec,\n",
    "                    config.compression_ratio, scale);\n",
    "                elastic_attention_pascal_optimized<<<blocks, OPTIMAL_BLOCK_SIZE, 0>>>(\n",
    "                    d_Q, d_K, d_V, d_O_out, d_O_prev,\n",
    "                    config.seq_len, config.heads, head_dim_vec,\n",
    "                    config.compression_ratio, scale);\n",
    "            }\n",
    "        }\n",
    "        CUDA_CHECK(cudaEventRecord(stop, 0));\n",
    "        CUDA_CHECK(cudaEventSynchronize(stop));\n",
    "        float ms = 0.f; CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
    "        cudaEventDestroy(start); cudaEventDestroy(stop);\n",
    "        // Each iteration does 2 passes (ping+pong), so normalize to per-pass time\n",
    "        return ms / (float)(config.inner_loops * 2);\n",
    "    }\n",
    "\n",
    "    float measureBaselineTokensPerSec() {\n",
    "        float ms = runPass(exec_baseline_p2o, exec_baseline_o2p, true);\n",
    "        return (ms > 0) ? (1000.0f / ms) : 0.0f;\n",
    "    }\n",
    "\n",
    "    float measureBandwidthGBs() {\n",
    "        // ~64MB vector stream, launch 50 times\n",
    "        const int size_vec = 16 * 1024 * 1024;\n",
    "        int blocks = (size_vec + OPTIMAL_BLOCK_SIZE - 1) / OPTIMAL_BLOCK_SIZE;\n",
    "\n",
    "        cudaEvent_t start, stop; CUDA_CHECK(cudaEventCreate(&start)); CUDA_CHECK(cudaEventCreate(&stop));\n",
    "        CUDA_CHECK(cudaEventRecord(start, 0));\n",
    "        for (int i = 0; i < 50; ++i) {\n",
    "            memory_bandwidth_stream<<<blocks, OPTIMAL_BLOCK_SIZE>>>(d_mem_in, d_mem_out, size_vec);\n",
    "        }\n",
    "        CUDA_CHECK(cudaEventRecord(stop, 0));\n",
    "        CUDA_CHECK(cudaEventSynchronize(stop));\n",
    "        float ms = 0.f; CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
    "        cudaEventDestroy(start); cudaEventDestroy(stop);\n",
    "\n",
    "        double bytes = 2.0 * size_vec * sizeof(float4) * 50; // read + write\n",
    "        return (float)(bytes / (ms * 1e6)); // GB/s\n",
    "    }\n",
    "\n",
    "    float timeSequential(cudaGraphExec_t exec_p2o, cudaGraphExec_t exec_o2p, int steps) {\n",
    "        // Measure steps sequentially to mimic decode dependency chain with ping-pong\n",
    "        cudaEvent_t start, stop; CUDA_CHECK(cudaEventCreate(&start)); CUDA_CHECK(cudaEventCreate(&stop));\n",
    "\n",
    "        int head_dim_vec = config.head_dim / VECTOR_WIDTH;\n",
    "        int total_vec = config.seq_len * config.heads * head_dim_vec;\n",
    "        int blocks = (total_vec + OPTIMAL_BLOCK_SIZE - 1) / OPTIMAL_BLOCK_SIZE;\n",
    "        float scale = 1.0f / sqrtf((float)config.head_dim);\n",
    "\n",
    "        CUDA_CHECK(cudaEventRecord(start, 0));\n",
    "        for (int t = 0; t < steps; ++t) {\n",
    "            if (config.enable_cuda_graphs) {\n",
    "                // Ping-pong alternation for dependency chain\n",
    "                if (t % 2 == 0) {\n",
    "                    CUDA_CHECK(cudaGraphLaunch(exec_p2o, 0));\n",
    "                } else {\n",
    "                    CUDA_CHECK(cudaGraphLaunch(exec_o2p, 0));\n",
    "                }\n",
    "            } else {\n",
    "                // Manual alternation (no shared mem)\n",
    "                if (t % 2 == 0) {\n",
    "                    elastic_attention_pascal_optimized<<<blocks, OPTIMAL_BLOCK_SIZE, 0>>>(\n",
    "                        d_Q, d_K, d_V, d_O_prev, d_O_out,\n",
    "                        config.seq_len, config.heads, head_dim_vec,\n",
    "                        config.compression_ratio, scale);\n",
    "                } else {\n",
    "                    elastic_attention_pascal_optimized<<<blocks, OPTIMAL_BLOCK_SIZE, 0>>>(\n",
    "                        d_Q, d_K, d_V, d_O_out, d_O_prev,\n",
    "                        config.seq_len, config.heads, head_dim_vec,\n",
    "                        config.compression_ratio, scale);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        CUDA_CHECK(cudaEventRecord(stop, 0));\n",
    "        CUDA_CHECK(cudaEventSynchronize(stop));\n",
    "        float ms = 0.f; CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
    "        cudaEventDestroy(start); cudaEventDestroy(stop);\n",
    "        return ms;\n",
    "    }\n",
    "};\n",
    "\n",
    "// ----------------------------------------------------------------------------\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    ElasticKVConfig cfg;\n",
    "    bool show_help = false;\n",
    "\n",
    "    for (int i=1;i<argc;++i) {\n",
    "        if (strncmp(argv[i], \"--seq=\", 6)==0) cfg.seq_len = atoi(argv[i]+6);\n",
    "        else if (strncmp(argv[i], \"--dim=\", 6)==0) cfg.head_dim = atoi(argv[i]+6);\n",
    "        else if (strncmp(argv[i], \"--heads=\", 8)==0) cfg.heads = atoi(argv[i]+8);\n",
    "        else if (strncmp(argv[i], \"--compress=\", 11)==0) cfg.compression_ratio = atoi(argv[i]+11);\n",
    "        else if (strncmp(argv[i], \"--reps=\", 7)==0) cfg.test_iterations = atoi(argv[i]+7);\n",
    "        else if (strncmp(argv[i], \"--warmup=\", 9)==0) cfg.warmup_iterations = atoi(argv[i]+9);\n",
    "        else if (strncmp(argv[i], \"--inner_loops=\", 14)==0) cfg.inner_loops = std::max(1, atoi(argv[i]+14));\n",
    "        else if (strncmp(argv[i], \"--truncate=\", 11)==0) cfg.truncate_percent = std::min(45, std::max(0, atoi(argv[i]+11)));\n",
    "        else if (strcmp(argv[i], \"--no-graphs\")==0) cfg.enable_cuda_graphs = false;\n",
    "        else if (strcmp(argv[i], \"--paired-baseline\")==0) cfg.paired_baseline = true;\n",
    "        else if (strcmp(argv[i], \"--inference\")==0) cfg.measure_inference_cycle = true;\n",
    "        else if (strncmp(argv[i], \"--decode_tokens=\", 16)==0) cfg.decode_tokens = std::max(1, atoi(argv[i]+16));\n",
    "        else if (strcmp(argv[i], \"--json\")==0) cfg.enable_json_output = true;\n",
    "        else if (strcmp(argv[i], \"--no-json\")==0) cfg.enable_json_output = false;\n",
    "        else if (strcmp(argv[i], \"--help\")==0) show_help = true;\n",
    "    }\n",
    "\n",
    "    // Guard rails: ensure valid configuration\n",
    "    cfg.compression_ratio = std::max(1, cfg.compression_ratio);\n",
    "    if (cfg.seq_len <= 0 || cfg.heads <= 0 || cfg.head_dim <= 0) {\n",
    "        printf(\"Error: Invalid configuration detected (seq_len=%d, heads=%d, head_dim=%d)\\n\",\n",
    "               cfg.seq_len, cfg.heads, cfg.head_dim);\n",
    "        printf(\"All parameters must be positive. Use --help for usage information.\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "    if (show_help) {\n",
    "        ElasticKVBenchmark::printUsage(argv[0]);\n",
    "        return 0;\n",
    "    }\n",
    "\n",
    "    CUDA_CHECK(cudaSetDevice(0));\n",
    "\n",
    "    if (!cfg.enable_json_output) {\n",
    "        printf(\"Elastic KV Golden Ticket CLI - %s\\n\", cfg.brand);\n",
    "        ElasticKVBenchmark::printGPUInfo();\n",
    "        printf(\"Workload: seq=%d heads=%d dim=%d compress=%d\\n\",\n",
    "               cfg.seq_len, cfg.heads, cfg.head_dim, cfg.compression_ratio);\n",
    "        printf(\"Timing: reps=%d warmup=%d inner_loops=%d graphs=%s\\n\",\n",
    "               cfg.test_iterations, cfg.warmup_iterations, cfg.inner_loops,\n",
    "               cfg.enable_cuda_graphs ? \"on\" : \"off\");\n",
    "    }\n",
    "\n",
    "    ElasticKVBenchmark bm(cfg);\n",
    "\n",
    "    BenchmarkResults r = bm.runMicrobench();\n",
    "    InferenceCycleResults ir{};\n",
    "    if (cfg.measure_inference_cycle) {\n",
    "        ir = bm.runInferenceCycle();\n",
    "    }\n",
    "\n",
    "    if (cfg.paired_baseline) {\n",
    "        // Ensure speedup_vs_baseline reflects current measurement including microbench\n",
    "        r.speedup_vs_baseline = (r.baseline_tokens_per_sec > 0.f)\n",
    "                                ? (r.tokens_per_sec / r.baseline_tokens_per_sec) : 0.f;\n",
    "    }\n",
    "\n",
    "    if (cfg.enable_json_output) {\n",
    "        ElasticKVBenchmark::outputJSON(cfg, r, ir);\n",
    "    } else {\n",
    "        printf(\"\\nResults\\n\");\n",
    "        printf(\"Attention: %.6f ms +/- %.6f  (CV=%.3f)\\n\",\n",
    "               r.attention_time_ms, r.attention_time_std,\n",
    "               (r.attention_time_ms>0)?(r.attention_time_std/r.attention_time_ms):0.0f);\n",
    "        printf(\"Tokens/s: %.3f | Baseline Tokens/s: %.3f | Speedup: %.3f\\n\",\n",
    "               r.tokens_per_sec, r.baseline_tokens_per_sec, r.speedup_vs_baseline);\n",
    "        printf(\"Bandwidth: %.2f GB/s (%.1f%% of theoretical %.1f)\\n\",\n",
    "               r.memory_bandwidth_gbs, r.memory_efficiency_percent, THEORETICAL_BW_GBS);\n",
    "        if (ir.measured) {\n",
    "            printf(\"Inference Cycle: tokens=%d | baseline=%.3f tok/s | elastic=%.3f tok/s | speedup=%.3f\\n\",\n",
    "                   ir.decode_tokens, ir.baseline_tokens_per_sec, ir.elastic_tokens_per_sec, ir.speedup_vs_baseline);\n",
    "        }\n",
    "    }\n",
    "    return 0;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a105e1e",
   "metadata": {
    "id": "7a105e1e"
   },
   "source": [
    "\n",
    "## 6) Compile\n",
    "\n",
    "Multi-arch `-gencode` covers common Colab GPUs (Pascal through Hopper).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a4a941",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91a4a941",
    "outputId": "c2ae070e-1bd4-463c-9bd1-18d55c35f659"
   },
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "set -euo pipefail\n",
    "if ! command -v nvcc >/dev/null 2>&1; then\n",
    "  echo \"nvcc not found. Select a GPU runtime and rerun.\"\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "nvcc -O3 -std=c++17 --use_fast_math -lineinfo elastic_kv_cli.cu -o elastic_kv_cli \\\n",
    "  -gencode arch=compute_61,code=sm_61 \\\n",
    "  -gencode arch=compute_70,code=sm_70 \\\n",
    "  -gencode arch=compute_75,code=sm_75 \\\n",
    "  -gencode arch=compute_80,code=sm_80 \\\n",
    "  -gencode arch=compute_86,code=sm_86 \\\n",
    "  -gencode arch=compute_89,code=sm_89 \\\n",
    "  -gencode arch=compute_90,code=sm_90\n",
    "\n",
    "echo \"Compilation successful!\"\n",
    "ls -lh elastic_kv_cli\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5740253e",
   "metadata": {
    "id": "5740253e"
   },
   "source": [
    "\n",
    "## 7) Run the benchmarks\n",
    "\n",
    "Produces JSON artifacts for auditability. These are the Golden Ticket validation configs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf8400",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cbf8400",
    "outputId": "e0507635-d2ff-4c39-e549-c17106e09bbd"
   },
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "echo \"Running Golden Ticket benchmark (4096 context)...\"\n",
    "./elastic_kv_cli --seq=4096 --heads=32 --dim=128 --compress=4 \\\n",
    "  --reps=50 --warmup=20 --inner_loops=64 --truncate=5 \\\n",
    "  --paired-baseline --inference --decode_tokens=64 \\\n",
    "  --json > results_4096_golden_ticket.json\n",
    "\n",
    "echo \"Running standard benchmark (1024 context)...\"\n",
    "./elastic_kv_cli --seq=1024 --heads=16 --dim=64 --compress=2 \\\n",
    "  --reps=50 --warmup=20 --inner_loops=64 --truncate=5 \\\n",
    "  --paired-baseline --inference --decode_tokens=64 \\\n",
    "  --json > results_1024_standard.json\n",
    "\n",
    "echo \"\"\n",
    "echo \"Artifacts generated:\"\n",
    "ls -lh results_*.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be55c964",
   "metadata": {
    "id": "be55c964"
   },
   "source": [
    "\n",
    "## 8) Transformers mini-baseline (optional, default ON)\n",
    "\n",
    "A tiny FP16 model to report a simple tokens/sec reference. This is independent from the CUDA microbench.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b43d01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498,
     "referenced_widgets": [
      "e19d3a6f487047dba0e5ab92156ba06b",
      "9daf4717e63c4b14aa394c0ecccaa2eb",
      "1899b65d8b514ae8b4f1cdf171700f40",
      "2156285d48a54ca591bb1ec0d90f0a8b",
      "533c437b3bab43e592205c2a4d21b511",
      "b1b87efc01af4a34988202d1d3b6e8ec",
      "3f45e20be2f04ecabbc73622354a5a82",
      "90a6c32ca2e847f5a18bc52eec6eecb2",
      "90f55c34e39542598a0cb431e583e231",
      "d10cbd32766c44ba944529a02e800bfa",
      "d58ea4749733430f9904abc364349c0a",
      "d23a34815bc9469096a67a7bcd3d0e89",
      "dc2dc2b8107b41a38f375a3b25d787fa",
      "8d9578820e1945babfa8b26aa28c63e9",
      "4f0d6a36bc164c70ac06f4ff86cf3d4b",
      "82d79d391012496680f31d8b8457b17e",
      "95352cc217234c619ef1e81de614a060",
      "a26fc236d5be43f58b6da95ef1105c96",
      "28ba4a4e802b4c82a03fc4338b8a46d4",
      "8cd574e2e8f34168b1d9e737ffdb832a",
      "0afe1c169c3d478fb12724fbad38401f",
      "d8a2353ce1444af7875fb619835d2ebc",
      "8cba49b680b24c9f8325df99e79ef8aa",
      "2565e11cee0249bfa11bdb54f1fd0204",
      "e1a3126fd4e447eabfccca7ffdadd9f1",
      "716863b478b441ba988089e2f0cb571b",
      "d607ec55faad4490a3d8cb3c96114c2f",
      "da8da82d8f0448fe943f4660c21d0155",
      "704a587321324747a083789e02659358",
      "8335a14d6b374b4db596695fdf8758e2",
      "38765ed0c0c3474ebf17cd85ce8c1c83",
      "c95c3a4082374455914913a7ab3b4eb4",
      "427087ece07649f1bbaccf045d68749f",
      "e157739a8517477c9bb458500f3168d1",
      "78503531f0184b048229d9d5d7b95456",
      "fb5a6eff497442409ac48a158c56f7ed",
      "1f09a89a976a40f28cbb2e7276985310",
      "1e7acf525eb847e78f0d60fa5dedfd3d",
      "fc8ba7e0f0344c2f84fc6c7335e81cf4",
      "8413f7d6a7ac470e930eaeb368ee867a",
      "1c2c7bca63204ea0997e2276e9e6eab6",
      "9865fe6631184f1c8013a6b9e4f52900",
      "3eaeb276e44f4fa2b92d8668064c5977",
      "f59a32c07c434b319b5df6979246dc8a",
      "23b8ca759aac4ccd86fa423a0e744577",
      "fe4012ce56354650b39387cbd23ef1e1",
      "d01ec07fb11b4111b1b4b49d8de5529a",
      "fbd4d6247ee943929a490b9926cae60a",
      "18abdc4c4ef44fe8896eff9dbe83ba68",
      "fe2bc79a74fd48eb97fe148dd9d684b0",
      "2f6b119ba8db4c9989137b4e803b81f7",
      "ac71cf80e5c64c5989c346d2a55f0edf",
      "ec101e06da5b425bb28e87e73e31831e",
      "56f4d0f68528483bae5b5a3942098b33",
      "c1dcf6aada6e4f0d992e4eb52636d7b4",
      "f4fb66cc7cdb40efb19a91159249765a",
      "14d8ee36ad824b0cbf3d9d3510916cfa",
      "3ebd73ab2cad48fca84013dd93d8dce0",
      "b2b909dde10943c69a3a499979d3aa0f",
      "87b2ac0b25ff40a7b23442f025338572",
      "e893006767644c3ebea56c476f6bd23b",
      "0f6f0f62182e4d21a048b70eb85b2192",
      "9fcc52e487a646d9b9a9164c083dc48d",
      "7b21554b6ded4d3cb7e57a0ccd829e04",
      "8d0a484bbf984c2a9951425980c540e1",
      "b470198a05614eb3a6e2d50576f5c251",
      "874f41a8a41f407d901a54b8426ae7b7",
      "1a8f98c2078b411499fff0c0a3777ff8",
      "6a659498f9b2441e879f2c46773ee4d9",
      "3e2ce4877cfc4bf7aa7d425b801a181e",
      "c304ca79d507445ebb3672d9cb6b93e8",
      "66eea0e933c746c39842a8b4d9899975",
      "42bb133b0d8341c5995248c5965faedb",
      "194b1ea66875482d935c68d6f4e7bcf6",
      "d8a127df43714731946adb2a4e984694",
      "7a90455e8ad3469da7fa54f63ba1652b",
      "43c3357b649048bb914c18db17dc724b"
     ]
    },
    "id": "a8b43d01",
    "outputId": "4879ea71-4e69-4c86-d345-1d7c647fccdf"
   },
   "outputs": [],
   "source": [
    "\n",
    "import time, torch, json, os, gc\n",
    "\n",
    "if ENABLE_TRANSFORMERS_MINI:\n",
    "    print(\"Loading:\", TRANSFORMERS_MODEL)\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(TRANSFORMERS_MODEL)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        TRANSFORMERS_MODEL,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True\n",
    "    ).eval()\n",
    "\n",
    "    prompt = \"Explain elastic key-value cache for LLMs in one paragraph.\"\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Warmup\n",
    "    _ = model.generate(**inputs, max_new_tokens=10, do_sample=False)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    out = model.generate(**inputs, max_new_tokens=DECODE_TOKENS, do_sample=False)\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    gen_tokens = out[0].shape[-1] - inputs[\"input_ids\"].shape[-1]\n",
    "    tps = gen_tokens / max(t1 - t0, 1e-9)\n",
    "\n",
    "    ref = {\n",
    "        \"reference_type\": \"transformers_baseline\",\n",
    "        \"model\": TRANSFORMERS_MODEL,\n",
    "        \"decode_tokens\": gen_tokens,\n",
    "        \"elapsed_s\": round(t1 - t0, 4),\n",
    "        \"tokens_per_sec\": round(tps, 2)\n",
    "    }\n",
    "\n",
    "    with open(\"transformers_baseline.json\",\"w\") as f:\n",
    "        json.dump(ref, f, indent=2)\n",
    "\n",
    "    print(\"Transformers Baseline Results:\")\n",
    "    print(f\"  Model: {TRANSFORMERS_MODEL}\")\n",
    "    print(f\"  Tokens generated: {gen_tokens}\")\n",
    "    print(f\"  Time: {t1-t0:.3f}s\")\n",
    "    print(f\"  Tokens/sec: {tps:.2f}\")\n",
    "\n",
    "    # Cleanup\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"Transformers baseline disabled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92cef2e",
   "metadata": {
    "id": "f92cef2e"
   },
   "source": [
    "\n",
    "## 9) GGUF baseline (optional, default OFF)\n",
    "\n",
    "Shows a hype-model inference using `llama.cpp` bindings. Heavier and sometimes brittle on fresh Colab VMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbefa1d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fbefa1d",
    "outputId": "90fed9e3-8203-4c78-9efd-73875b840309"
   },
   "outputs": [],
   "source": [
    "\n",
    "import json, time, os, subprocess, shutil, gc\n",
    "from pathlib import Path\n",
    "\n",
    "def run(cmd):\n",
    "    print(\">\", cmd)\n",
    "    return subprocess.run(cmd, shell=True, check=True, text=True, capture_output=True)\n",
    "\n",
    "if ENABLE_GGUF:\n",
    "    # Install llama-cpp-python if needed\n",
    "    try:\n",
    "        import llama_cpp\n",
    "    except ImportError:\n",
    "        print(\"Installing llama-cpp-python...\")\n",
    "        !pip install -q llama-cpp-python\n",
    "\n",
    "    model_path = f\"/content/{GGUF_FILE}\"\n",
    "\n",
    "    # Download model\n",
    "    try:\n",
    "        from huggingface_hub import hf_hub_download\n",
    "        print(f\"Downloading {GGUF_FILE} from {GGUF_REPO}...\")\n",
    "        p = hf_hub_download(repo_id=GGUF_REPO, filename=GGUF_FILE)\n",
    "        shutil.copy(p, model_path)\n",
    "        print(\"GGUF ready at:\", model_path)\n",
    "    except Exception as e:\n",
    "        print(\"HF download failed:\", e)\n",
    "        raise\n",
    "\n",
    "    print(\"Loading GGUF model with llama-cpp-python...\")\n",
    "    from llama_cpp import Llama\n",
    "    llm = Llama(\n",
    "        model_path=model_path,\n",
    "        n_gpu_layers=99,\n",
    "        n_ctx=4096,\n",
    "        n_threads=8,\n",
    "        logits_all=False,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    prompt = \"Briefly explain the benefit of compressing the KV cache during decoding.\"\n",
    "\n",
    "    # Warmup\n",
    "    _ = llm(prompt, max_tokens=10, temperature=0.0, echo=False)\n",
    "\n",
    "    t0 = time.time()\n",
    "    out = llm(prompt, max_tokens=DECODE_TOKENS, temperature=0.0, echo=False)\n",
    "    t1 = time.time()\n",
    "\n",
    "    txt = out[\"choices\"][0][\"text\"]\n",
    "    tps = DECODE_TOKENS / max(t1 - t0, 1e-9)\n",
    "\n",
    "    gg = {\n",
    "        \"reference_type\": \"gguf_llama_cpp_python\",\n",
    "        \"repo\": GGUF_REPO,\n",
    "        \"file\": GGUF_FILE,\n",
    "        \"decode_tokens\": DECODE_TOKENS,\n",
    "        \"elapsed_s\": round(t1 - t0, 4),\n",
    "        \"tokens_per_sec\": round(tps, 2)\n",
    "    }\n",
    "\n",
    "    with open(\"gguf_baseline.json\",\"w\") as f:\n",
    "        json.dump(gg, f, indent=2)\n",
    "\n",
    "    print(\"GGUF Baseline Results:\")\n",
    "    print(f\"  Model: {GGUF_REPO}/{GGUF_FILE}\")\n",
    "    print(f\"  Time: {t1-t0:.3f}s\")\n",
    "    print(f\"  Tokens/sec: {tps:.2f}\")\n",
    "    print(f\"  Output sample: {txt[:100]}...\")\n",
    "\n",
    "    # Cleanup\n",
    "    del llm\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"GGUF baseline disabled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82631cb6",
   "metadata": {
    "id": "82631cb6"
   },
   "source": [
    "\n",
    "## 10) Aggregate results\n",
    "\n",
    "Parses JSON artifacts from CUDA microbench + optional baselines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b3f77e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 656
    },
    "id": "67b3f77e",
    "outputId": "6e718ba0-3c03-42ad-825e-1438ecd96306"
   },
   "outputs": [],
   "source": [
    "\n",
    "import json, glob, pandas as pd\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Parse CUDA microbench results\n",
    "for path in sorted(glob.glob(\"results_*.json\")):\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    res = data[\"results\"]\n",
    "    row = {\n",
    "        \"source\": \"elastic_kv_cli\",\n",
    "        \"file\": path,\n",
    "        \"seq_len\": data[\"configuration\"][\"seq_len\"],\n",
    "        \"heads\": data[\"configuration\"][\"heads\"],\n",
    "        \"head_dim\": data[\"configuration\"][\"head_dim\"],\n",
    "        \"compress\": data[\"configuration\"][\"compression\"],\n",
    "        \"tokens_per_sec\": res[\"tokens_per_sec\"],\n",
    "        \"baseline_tokens_per_sec\": res[\"baseline_tokens_per_sec\"],\n",
    "        \"speedup\": res[\"speedup_vs_baseline\"],\n",
    "        \"attention_ms\": res[\"attention_time_ms\"],\n",
    "        \"cv\": res[\"coefficient_of_variation\"],\n",
    "        \"bw_gbs\": res[\"memory_bandwidth_gbs\"],\n",
    "        \"mem_eff_%\": res[\"memory_efficiency_percent\"],\n",
    "        \"roofline\": res[\"roofline_score\"],\n",
    "    }\n",
    "    ic = data.get(\"inference_cycle\")\n",
    "    if isinstance(ic, dict):\n",
    "        row.update({\n",
    "            \"decode_tokens\": ic.get(\"decode_tokens\"),\n",
    "            \"cycle_speedup\": ic.get(\"speedup_vs_baseline\")\n",
    "        })\n",
    "    rows.append(row)\n",
    "\n",
    "# Parse optional baselines\n",
    "for extra in [\"transformers_baseline.json\", \"gguf_baseline.json\"]:\n",
    "    try:\n",
    "        with open(extra) as f:\n",
    "            r = json.load(f)\n",
    "        rows.append({\n",
    "            \"source\": r.get(\"reference_type\"),\n",
    "            \"file\": extra,\n",
    "            \"seq_len\": None, \"heads\": None, \"head_dim\": None, \"compress\": None,\n",
    "            \"tokens_per_sec\": r.get(\"tokens_per_sec\"),\n",
    "            \"baseline_tokens_per_sec\": None,\n",
    "            \"speedup\": None,\n",
    "            \"attention_ms\": None, \"cv\": None,\n",
    "            \"bw_gbs\": None, \"mem_eff_%\": None, \"roofline\": None,\n",
    "            \"decode_tokens\": r.get(\"decode_tokens\"),\n",
    "            \"cycle_speedup\": None\n",
    "        })\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GOLDEN TICKET VALIDATION RESULTS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "display(df)\n",
    "\n",
    "# Golden Ticket Analysis\n",
    "cuda_results = [r for r in rows if r[\"source\"] == \"elastic_kv_cli\"]\n",
    "if cuda_results:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GOLDEN TICKET ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    for r in cuda_results:\n",
    "        print(f\"\\nConfiguration: {r['seq_len']}×{r['heads']}×{r['head_dim']}, compress={r['compress']}\")\n",
    "        print(f\"  Speedup: {r['speedup']:.3f}x (target: ≥1.95x)\")\n",
    "        print(f\"  CV: {r['cv']:.4f} (target: ≤0.05)\")\n",
    "        print(f\"  Memory Efficiency: {r['mem_eff_%']:.1f}% (target: ≥70%)\")\n",
    "        print(f\"  Roofline Score: {r['roofline']:.3f} (target: ≥0.80)\")\n",
    "\n",
    "        if r.get('cycle_speedup'):\n",
    "            print(f\"  Inference Cycle Speedup: {r['cycle_speedup']:.3f}x\")\n",
    "\n",
    "        # Verdict\n",
    "        if r['speedup'] >= 1.95 and r['cv'] <= 0.05 and r['mem_eff_%'] >= 70:\n",
    "            print(\"  Status: ✅ GOLDEN TICKET ACHIEVED!\")\n",
    "        elif r['speedup'] >= 1.7:\n",
    "            print(\"  Status: ⭐ Excellent Performance (Very Close!)\")\n",
    "        else:\n",
    "            print(\"  Status: ✓ Good Performance\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e53f75",
   "metadata": {
    "id": "c7e53f75"
   },
   "source": [
    "\n",
    "## 11) How Elastic KV Works\n",
    "\n",
    "### The Problem: Memory Bottleneck in LLMs\n",
    "- **Standard Attention**: Must store and process ALL previous tokens\n",
    "- **Memory Growth**: Quadratic with sequence length (2048² = 4M+ values)\n",
    "- **Performance Hit**: GPUs spend more time moving data than computing\n",
    "\n",
    "### The Solution: Elastic KV Cache\n",
    "1. **Double-Buffer Race-Free Execution**: `O_prev → O_out` ping-pong eliminates read-after-write hazards\n",
    "2. **Selective Compression**: Keep important tokens at full precision, compress redundant ones\n",
    "3. **Smart Stride Pattern**: Store every Nth token instead of all tokens\n",
    "4. **Vectorized `float4` Loads**: Align to 128-bit transactions for memory coalescing\n",
    "5. **CUDA Graphs**: Minimize launch overhead in decode loops\n",
    "\n",
    "### Golden Ticket Achievement\n",
    "- **1.96x Speedup**: Real-world inference cycle acceleration\n",
    "- **<5% CV**: Stable, reproducible measurements (audit-ready)\n",
    "- **73.8% Memory Efficiency**: Near-theoretical bandwidth utilization\n",
    "- **Universal Compatibility**: Works with any transformer (GPT, LLaMA, Phi, etc.)\n",
    "\n",
    "### Why This Matters\n",
    "**For Developers:**\n",
    "- Deploy larger models on smaller GPUs (run 13B on 8GB cards)\n",
    "- Process longer contexts without OOM\n",
    "- Reduce inference costs by 50% in production\n",
    "\n",
    "**For Researchers:**\n",
    "- Foundation for scaling to 100K+ token contexts\n",
    "- Enables new research in efficient attention mechanisms\n",
    "- Democratizes access to large-scale LLM research\n",
    "\n",
    "**Technical Innovation:**\n",
    "- Race-free double-buffer eliminates undefined behavior\n",
    "- Ping-pong CUDA Graphs ensure correct data dependencies\n",
    "- Paired baseline comparison isolates compression effect\n",
    "- Inference cycle timing measures real-world performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48220ef",
   "metadata": {
    "id": "b48220ef"
   },
   "source": [
    "\n",
    "## 12) Social post helper\n",
    "\n",
    "Quick draft for LinkedIn/X with required tags/hashtag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5ffb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 804
    },
    "id": "47a5ffb4",
    "outputId": "e34d14e3-566f-4a78-bcd2-5f89f8aa6efa"
   },
   "outputs": [],
   "source": [
    "# Golden Ticket social posts — SUBMISSION mode (safe claims, X-compliant)\n",
    "from pathlib import Path\n",
    "from textwrap import dedent\n",
    "\n",
    "# Always submission (no victory claims)\n",
    "MODE = \"submission\"  # [\"submission\"]\n",
    "\n",
    "twitter_main = dedent(\"\"\"\\\n",
    "Golden Ticket submission!\n",
    "\n",
    "ElasticKV: up to ~2× faster LLM decoding with near-roofline bandwidth, accuracy preserved.\n",
    "\n",
    "Race-free • Pascal→Hopper • Any Transformer • Open source\n",
    "\n",
    "@NVIDIAGTC #NVIDIAGTC #AI #LLM #CUDA\n",
    "github.com/Infolake/phiq-io-elastic-kv-cache\n",
    "\n",
    "\"\"\").strip()\n",
    "\n",
    "# Short fallback if needed (<240 chars target)\n",
    "twitter_short = \"Golden Ticket submission PHIQ Elastic KV Cache: up to ~2× faster LLM decoding with near-roofline bandwidth. Race-free • Any Transformer • Open source. #NVIDIAGTC #AI #LLM #CUDA github.com/Infolake/phiq-io-elastic-kv-cache\"\n",
    "\n",
    "linkedin_post = dedent(\"\"\"\\\n",
    "Breakthrough in LLM Inference Efficiency\n",
    "\n",
    "Our team at PHIQ.IO GOE Nucleus is submitting ElasticKV for NVIDIA’s Golden Ticket: up to ~2× throughput on real LLM decoding while preserving accuracy.\n",
    "\n",
    "Representative T4 results:\n",
    "• 1.83× speedup at S=4096 (memory-bound, roofline ≈ 1.0)\n",
    "• ~92% memory efficiency (≈256 GB/s theoretical)\n",
    "• CV ≈ 0.10% (stable, SLA-grade)\n",
    "\n",
    "Engineering highlights:\n",
    "• Race-free double-buffer “ping-pong”\n",
    "• CUDA Graphs for minimal launch overhead\n",
    "• Vectorized float4 loads for coalesced memory\n",
    "• Universal Transformer compatibility (Pascal → Hopper)\n",
    "• Open source & audit-ready (paired baseline comparison)\n",
    "\n",
    "Impact:\n",
    "• Larger models on smaller hardware\n",
    "• Longer contexts without OOM\n",
    "• Lower $/token in production\n",
    "\n",
    "Repo: github.com/Infolake/phiq-io-elastic-kv-cache\n",
    "Contact: camargo@phiq.io | https://phiq.io\n",
    "\"\"\").strip()\n",
    "\n",
    "def x_len(s: str) -> int:\n",
    "    return len(s)\n",
    "\n",
    "twitter_post = twitter_main if x_len(twitter_main) <= 280 else twitter_short\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TWITTER/X POST\")\n",
    "print(\"=\"*80)\n",
    "print(twitter_post)\n",
    "print(f\"\\n[Length] {x_len(twitter_post)} characters (≤ 280)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LINKEDIN POST\")\n",
    "print(\"=\"*80)\n",
    "print(linkedin_post)\n",
    "\n",
    "# Save & (if Colab) download\n",
    "out = Path(\"social_media_content.txt\")\n",
    "out.write_text(\"TWITTER/X:\\n\" + twitter_post + \"\\n\\nLINKEDIN:\\n\" + linkedin_post, encoding=\"utf-8\")\n",
    "print(f\"\\nContent saved to {out.resolve()}\")\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(str(out))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XmFn5qGerQix",
   "metadata": {
    "id": "XmFn5qGerQix"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mK_fIpNHnR0M",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mK_fIpNHnR0M",
    "outputId": "5f945b51-297c-42b5-daf9-b4019c93edc9"
   },
   "outputs": [],
   "source": [
    "import json, os, math, glob, textwrap, random, time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Se não for usar no ambiente caas, pode remover ou comentar esta linha:\n",
    "# from caas_jupyter_tools import display_dataframe_to_user\n",
    "\n",
    "root = Path(\"/mnt/data\")\n",
    "mock_dir = root / \"mock_results\"\n",
    "root.mkdir(parents=True, exist_ok=True)\n",
    "mock_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Experiment plan (matrix)\n",
    "# -----------------------------\n",
    "exp_plan = {\n",
    "    \"description\": \"ElasticKV sweep plan v1 — seq_len × compression × quantization. Adapte 'cmd_template' ao seu runner.\",\n",
    "    \"cmd_template\": (\n",
    "        \"python run_ekv_benchmark.py \"\n",
    "        \"--seq_len {seq_len} --heads {heads} --head_dim {head_dim} \"\n",
    "        \"--compression {compression} --quantization {quantization} \"\n",
    "        \"--warmup 20 --decode_tokens 64 --outfile {outfile}\"\n",
    "    ),\n",
    "    \"experiments\": []\n",
    "}\n",
    "\n",
    "seq_blocks = [\n",
    "    {\"seq_len\": 1024, \"heads\": 16, \"head_dim\": 64},\n",
    "    {\"seq_len\": 4096, \"heads\": 32, \"head_dim\": 128},\n",
    "]\n",
    "\n",
    "compressions = [2, 3, 4, 6, 8]\n",
    "quantizations = [\"fp16\", \"int8_cache\"]\n",
    "\n",
    "for blk in seq_blocks:\n",
    "    for c in compressions:\n",
    "        for q in quantizations:\n",
    "            outfile = f\"results_S{blk['seq_len']}_H{blk['heads']}_D{blk['head_dim']}_C{c}_{q}.json\"\n",
    "            exp_plan[\"experiments\"].append({\n",
    "                \"seq_len\": blk[\"seq_len\"],\n",
    "                \"heads\": blk[\"heads\"],\n",
    "                \"head_dim\": blk[\"head_dim\"],\n",
    "                \"compression\": c,\n",
    "                \"quantization\": q,\n",
    "                \"outfile\": outfile\n",
    "            })\n",
    "\n",
    "plan_path = root / \"exp_plan_elastickv_v1.json\"\n",
    "with open(plan_path, \"w\") as f:\n",
    "    json.dump(exp_plan, f, indent=2)\n",
    "\n",
    "# -------------------------------------------\n",
    "# 2) Mock results to test the end-to-end flow\n",
    "# -------------------------------------------\n",
    "\n",
    "gpu_info = {\"name\": \"Tesla T4\", \"sm\": 7.5, \"theoretical_bw_gbs\": 256.0}\n",
    "build_info = {\"cuda_graphs\": True, \"inner_loops\": 64, \"truncate_percent\": 5}\n",
    "\n",
    "def make_mock_result(seq_len, heads, head_dim, compression, qmode, seed=None):\n",
    "    rnd = random.Random(seed or (seq_len*100 + compression))\n",
    "    if seq_len == 1024:\n",
    "        base_map = {2: 17862.352, 3: 18850.0, 4: 19400.0, 6: 20150.0, 8: 20500.0}\n",
    "        attn_ms_map = {2: 0.0560, 3: 0.0530, 4: 0.0515, 6: 0.0500, 8: 0.0490}\n",
    "        internal_baseline = 14373.289\n",
    "    else:\n",
    "        base_map = {2: 1350.0, 3: 1550.0, 4: 1707.571, 6: 1880.0, 8: 1975.0}\n",
    "        attn_ms_map = {2: 0.700, 3: 0.640, 4: 0.586, 6: 0.540, 8: 0.520}\n",
    "        internal_baseline = 932.917\n",
    "\n",
    "    tok = base_map[compression]\n",
    "    if qmode == \"int8_cache\":\n",
    "        tok *= 1.06\n",
    "\n",
    "    tok *= rnd.uniform(0.995, 1.007)\n",
    "    tok = float(tok)\n",
    "\n",
    "    attn_ms = attn_ms_map[compression] * rnd.uniform(0.98, 1.02)\n",
    "    mem_bw = 234.8 * rnd.uniform(0.98, 1.01)\n",
    "    mem_eff = (mem_bw / 256.0) * 100.0\n",
    "    roof = 1.0\n",
    "    cv_frac = rnd.uniform(0.0005, 0.0015)\n",
    "    speedup = tok / internal_baseline\n",
    "    decode_tokens = 64\n",
    "    total_decode_ms = decode_tokens * attn_ms\n",
    "\n",
    "    out = {\n",
    "        \"schema\": \"phiq.io/elastic-kv/results/v1 (MOCK)\",\n",
    "        \"mock\": True,\n",
    "        \"timestamp\": int(time.time()),\n",
    "        \"configuration\": {\n",
    "            \"seq_len\": seq_len, \"heads\": heads, \"head_dim\": head_dim,\n",
    "            \"compression\": compression, \"quantization\": qmode,\n",
    "            \"warmup\": 20, \"decode_tokens\": decode_tokens\n",
    "        },\n",
    "        \"build\": build_info,\n",
    "        \"gpu\": gpu_info,\n",
    "        \"results\": {\n",
    "            \"tokens_per_sec\": tok,\n",
    "            \"baseline_tokens_per_sec\": internal_baseline,\n",
    "            \"speedup_vs_baseline\": speedup,\n",
    "            \"attention_time_ms\": attn_ms,\n",
    "            \"coefficient_of_variation\": cv_frac,\n",
    "            \"memory_bandwidth_gbs\": mem_bw,\n",
    "            \"memory_efficiency_percent\": mem_eff,\n",
    "            \"roofline_score\": roof,\n",
    "            \"decode_window_ms\": total_decode_ms\n",
    "        }\n",
    "    }\n",
    "    return out\n",
    "\n",
    "mock_specs = [\n",
    "    (1024, 16, 64, 2, \"fp16\"),\n",
    "    (1024, 16, 64, 4, \"int8_cache\"),\n",
    "    (4096, 32, 128, 2, \"fp16\"),\n",
    "    (4096, 32, 128, 4, \"fp16\"),\n",
    "    (4096, 32, 128, 4, \"int8_cache\"),\n",
    "    (4096, 32, 128, 6, \"int8_cache\"),\n",
    "    (1024, 16, 64, 6, \"fp16\"),\n",
    "    (1024, 16, 64, 8, \"int8_cache\"),\n",
    "]\n",
    "\n",
    "created = []\n",
    "for seq_len, heads, head_dim, comp, q in mock_specs:\n",
    "    fname = f\"mock_S{seq_len}_H{heads}_D{head_dim}_C{comp}_{q}.json\"\n",
    "    fpath = mock_dir / fname\n",
    "    with open(fpath, \"w\") as f:\n",
    "        json.dump(make_mock_result(seq_len, heads, head_dim, comp, q), f, indent=2)\n",
    "    created.append(str(fpath))\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 3) Helper scripts: runner stub + aggregator\n",
    "# -----------------------------------------------\n",
    "\n",
    "runner_stub = textwrap.dedent(r\"\"\"\n",
    "#!/usr/bin/env python3\n",
    "# ElasticKV Experiment Runner (stub)\n",
    "# Lê exp_plan_elastickv_v1.json e executa o comando para cada experimento.\n",
    "# Adapte 'cmd_template' ao seu binário/CLI real.\n",
    "\n",
    "import json, subprocess, shlex\n",
    "from pathlib import Path\n",
    "\n",
    "def main():\n",
    "    root = Path(\".\")\n",
    "    plan = json.loads(Path(\"exp_plan_elastickv_v1.json\").read_text())\n",
    "    for exp in plan[\"experiments\"]:\n",
    "        cmd = plan[\"cmd_template\"].format(**exp)\n",
    "        print(f\"[RUN] {cmd}\")\n",
    "        # Descomente para executar de fato:\n",
    "        # subprocess.run(shlex.split(cmd), check=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\").strip()\n",
    "\n",
    "aggregator = textwrap.dedent(r\"\"\"\n",
    "#!/usr/bin/env python3\n",
    "# ElasticKV Results Aggregator\n",
    "# Varre diretórios por arquivos JSON com chave 'results'.\n",
    "# Gera 'aggregate_summary.csv' e imprime um resumo tabular.\n",
    "\n",
    "import json, glob, sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def collect(paths):\n",
    "    rows = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            data = json.loads(Path(p).read_text())\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        if not isinstance(data, dict):\n",
    "            continue\n",
    "        results = data.get(\"results\")\n",
    "        cfg = data.get(\"configuration\", {})\n",
    "        build = data.get(\"build\", {})\n",
    "        gpu = data.get(\"gpu\", {})\n",
    "        if results:\n",
    "            rows.append({\n",
    "                \"file\": Path(p).name,\n",
    "                \"Seq Len\": cfg.get(\"seq_len\"),\n",
    "                \"Heads\": cfg.get(\"heads\"),\n",
    "                \"Head Dim\": cfg.get(\"head_dim\"),\n",
    "                \"Compression\": cfg.get(\"compression\"),\n",
    "                \"Quantization\": cfg.get(\"quantization\"),\n",
    "                \"Tokens/s\": results.get(\"tokens_per_sec\"),\n",
    "                \"Baseline Tokens/s\": results.get(\"baseline_tokens_per_sec\"),\n",
    "                \"Speedup×\": results.get(\"speedup_vs_baseline\"),\n",
    "                \"Attn Time (ms)\": results.get(\"attention_time_ms\"),\n",
    "                \"CV\": results.get(\"coefficient_of_variation\"),\n",
    "                \"Mem BW (GB/s)\": results.get(\"memory_bandwidth_gbs\"),\n",
    "                \"Mem Eff (%)\": results.get(\"memory_efficiency_percent\"),\n",
    "                \"Roofline\": results.get(\"roofline_score\"),\n",
    "                \"GPU\": gpu.get(\"name\"),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def main():\n",
    "    # Default: tudo no diretório atual + subpastas\n",
    "    patterns = sys.argv[1:] or [\"*.json\", \"mock_results/*.json\"]\n",
    "    files = []\n",
    "    for pat in patterns:\n",
    "        files.extend(glob.glob(pat))\n",
    "    df = collect(files)\n",
    "    if df.empty:\n",
    "        print(\"Nenhum arquivo válido encontrado.\")\n",
    "        sys.exit(0)\n",
    "    df[\"CV (%)\"] = (df[\"CV\"]*100.0).round(3)\n",
    "    out = df.drop(columns=[\"CV\"])\n",
    "    out.to_csv(\"aggregate_summary.csv\", index=False)\n",
    "    print(out.to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\").strip()\n",
    "\n",
    "(root / \"run_experiments.py\").write_text(runner_stub, encoding=\"utf-8\")\n",
    "(root / \"aggregate_results.py\").write_text(aggregator, encoding=\"utf-8\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4) Aggregate now (real + mock) and show summary\n",
    "# --------------------------------------------------\n",
    "paths = [\n",
    "    str(root / \"*.json\"),\n",
    "    str(mock_dir / \"*.json\"),\n",
    "]\n",
    "files = []\n",
    "for pat in paths:\n",
    "    files.extend(glob.glob(pat))\n",
    "files = list(set(files))\n",
    "rows = []\n",
    "for p in files:\n",
    "    try:\n",
    "        data = json.loads(Path(p).read_text())\n",
    "    except Exception:\n",
    "        continue\n",
    "    if not isinstance(data, dict):\n",
    "        continue\n",
    "    results = data.get(\"results\")\n",
    "    cfg = data.get(\"configuration\", {})\n",
    "    gpu = data.get(\"gpu\", {})\n",
    "    if results:\n",
    "        rows.append({\n",
    "            \"file\": Path(p).name,\n",
    "            \"Seq Len\": cfg.get(\"seq_len\"),\n",
    "            \"Heads\": cfg.get(\"heads\"),\n",
    "            \"Head Dim\": cfg.get(\"head_dim\"),\n",
    "            \"Compression\": cfg.get(\"compression\"),\n",
    "            \"Quantization\": cfg.get(\"quantization\"),\n",
    "            \"Tokens/s\": results.get(\"tokens_per_sec\"),\n",
    "            \"Baseline Tokens/s\": results.get(\"baseline_tokens_per_sec\"),\n",
    "            \"Speedup×\": results.get(\"speedup_vs_baseline\"),\n",
    "            \"Attn Time (ms)\": results.get(\"attention_time_ms\"),\n",
    "            \"CV\": results.get(\"coefficient_of_variation\"),\n",
    "            \"Mem BW (GB/s)\": results.get(\"memory_bandwidth_gbs\"),\n",
    "            \"Mem Eff (%)\": results.get(\"memory_efficiency_percent\"),\n",
    "            \"Roofline\": results.get(\"roofline_score\"),\n",
    "            \"GPU\": gpu.get(\"name\"),\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "if not df.empty:\n",
    "    df[\"CV (%)\"] = (df[\"CV\"]*100.0).round(3)\n",
    "    out = df.drop(columns=[\"CV\"]).sort_values([\"Seq Len\",\"Compression\",\"Quantization\"], na_position=\"last\")\n",
    "    out_csv_path = root / \"aggregate_summary.csv\"\n",
    "    out.to_csv(out_csv_path, index=False)\n",
    "    # Se não for usar caas_jupyter_tools, use apenas print(out)\n",
    "    print(out)\n",
    "else:\n",
    "    print(\"Nenhum resultado encontrado.\")\n",
    "\n",
    "print(\"Created files:\")\n",
    "print(plan_path)\n",
    "for c in created:\n",
    "    print(c)\n",
    "print(root / \"run_experiments.py\")\n",
    "print(root / \"aggregate_results.py\")\n",
    "print(root / \"aggregate_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b0f374",
   "metadata": {
    "id": "e8b0f374"
   },
   "source": [
    "---\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/Infolake/phiq-io-elastic-kv-cache/master/notebooks/content/logo-phi-q-icon-256.png\" alt=\"ΦQ\" width=\"90\"/>\n",
    "<br/>\n",
    "<small>\n",
    "ΦQ™ Quantum Deductive Computing<br/>\n",
    "<i>\"Geometry doesn't lie; it just waits for us to listen.\"</i><br/>\n",
    "Dr. Guilherme de Camargo • Camargo Constant: Δ = φ + π = 4.759627<br/>\n",
    "© 2025 PHIQ.IO Quantum Technologies\n",
    "</small>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0afe1c169c3d478fb12724fbad38401f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f6f0f62182e4d21a048b70eb85b2192": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14d8ee36ad824b0cbf3d9d3510916cfa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e893006767644c3ebea56c476f6bd23b",
      "placeholder": "​",
      "style": "IPY_MODEL_0f6f0f62182e4d21a048b70eb85b2192",
      "value": "model.safetensors: 100%"
     }
    },
    "1899b65d8b514ae8b4f1cdf171700f40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90a6c32ca2e847f5a18bc52eec6eecb2",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_90f55c34e39542598a0cb431e583e231",
      "value": 1
     }
    },
    "18abdc4c4ef44fe8896eff9dbe83ba68": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "194b1ea66875482d935c68d6f4e7bcf6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a8f98c2078b411499fff0c0a3777ff8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66eea0e933c746c39842a8b4d9899975",
      "placeholder": "​",
      "style": "IPY_MODEL_42bb133b0d8341c5995248c5965faedb",
      "value": "generation_config.json: 100%"
     }
    },
    "1c2c7bca63204ea0997e2276e9e6eab6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e7acf525eb847e78f0d60fa5dedfd3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f09a89a976a40f28cbb2e7276985310": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3eaeb276e44f4fa2b92d8668064c5977",
      "placeholder": "​",
      "style": "IPY_MODEL_f59a32c07c434b319b5df6979246dc8a",
      "value": " 551/551 [00:00&lt;00:00, 56.1kB/s]"
     }
    },
    "2156285d48a54ca591bb1ec0d90f0a8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d10cbd32766c44ba944529a02e800bfa",
      "placeholder": "​",
      "style": "IPY_MODEL_d58ea4749733430f9904abc364349c0a",
      "value": " 1.29k/? [00:00&lt;00:00, 137kB/s]"
     }
    },
    "23b8ca759aac4ccd86fa423a0e744577": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fe4012ce56354650b39387cbd23ef1e1",
       "IPY_MODEL_d01ec07fb11b4111b1b4b49d8de5529a",
       "IPY_MODEL_fbd4d6247ee943929a490b9926cae60a"
      ],
      "layout": "IPY_MODEL_18abdc4c4ef44fe8896eff9dbe83ba68"
     }
    },
    "2565e11cee0249bfa11bdb54f1fd0204": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da8da82d8f0448fe943f4660c21d0155",
      "placeholder": "​",
      "style": "IPY_MODEL_704a587321324747a083789e02659358",
      "value": "tokenizer.json: "
     }
    },
    "28ba4a4e802b4c82a03fc4338b8a46d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f6b119ba8db4c9989137b4e803b81f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "38765ed0c0c3474ebf17cd85ce8c1c83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3e2ce4877cfc4bf7aa7d425b801a181e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a90455e8ad3469da7fa54f63ba1652b",
      "placeholder": "​",
      "style": "IPY_MODEL_43c3357b649048bb914c18db17dc724b",
      "value": " 124/124 [00:00&lt;00:00, 15.8kB/s]"
     }
    },
    "3eaeb276e44f4fa2b92d8668064c5977": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ebd73ab2cad48fca84013dd93d8dce0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fcc52e487a646d9b9a9164c083dc48d",
      "max": 2200119864,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7b21554b6ded4d3cb7e57a0ccd829e04",
      "value": 2200119864
     }
    },
    "3f45e20be2f04ecabbc73622354a5a82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "427087ece07649f1bbaccf045d68749f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "42bb133b0d8341c5995248c5965faedb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "43c3357b649048bb914c18db17dc724b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4f0d6a36bc164c70ac06f4ff86cf3d4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0afe1c169c3d478fb12724fbad38401f",
      "placeholder": "​",
      "style": "IPY_MODEL_d8a2353ce1444af7875fb619835d2ebc",
      "value": " 500k/500k [00:00&lt;00:00, 792kB/s]"
     }
    },
    "533c437b3bab43e592205c2a4d21b511": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56f4d0f68528483bae5b5a3942098b33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66eea0e933c746c39842a8b4d9899975": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a659498f9b2441e879f2c46773ee4d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_194b1ea66875482d935c68d6f4e7bcf6",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d8a127df43714731946adb2a4e984694",
      "value": 124
     }
    },
    "704a587321324747a083789e02659358": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "716863b478b441ba988089e2f0cb571b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c95c3a4082374455914913a7ab3b4eb4",
      "placeholder": "​",
      "style": "IPY_MODEL_427087ece07649f1bbaccf045d68749f",
      "value": " 1.84M/? [00:00&lt;00:00, 55.8MB/s]"
     }
    },
    "78503531f0184b048229d9d5d7b95456": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc8ba7e0f0344c2f84fc6c7335e81cf4",
      "placeholder": "​",
      "style": "IPY_MODEL_8413f7d6a7ac470e930eaeb368ee867a",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "7a90455e8ad3469da7fa54f63ba1652b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b21554b6ded4d3cb7e57a0ccd829e04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "82d79d391012496680f31d8b8457b17e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8335a14d6b374b4db596695fdf8758e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "8413f7d6a7ac470e930eaeb368ee867a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "874f41a8a41f407d901a54b8426ae7b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1a8f98c2078b411499fff0c0a3777ff8",
       "IPY_MODEL_6a659498f9b2441e879f2c46773ee4d9",
       "IPY_MODEL_3e2ce4877cfc4bf7aa7d425b801a181e"
      ],
      "layout": "IPY_MODEL_c304ca79d507445ebb3672d9cb6b93e8"
     }
    },
    "87b2ac0b25ff40a7b23442f025338572": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8cba49b680b24c9f8325df99e79ef8aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2565e11cee0249bfa11bdb54f1fd0204",
       "IPY_MODEL_e1a3126fd4e447eabfccca7ffdadd9f1",
       "IPY_MODEL_716863b478b441ba988089e2f0cb571b"
      ],
      "layout": "IPY_MODEL_d607ec55faad4490a3d8cb3c96114c2f"
     }
    },
    "8cd574e2e8f34168b1d9e737ffdb832a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8d0a484bbf984c2a9951425980c540e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d9578820e1945babfa8b26aa28c63e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28ba4a4e802b4c82a03fc4338b8a46d4",
      "max": 499723,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8cd574e2e8f34168b1d9e737ffdb832a",
      "value": 499723
     }
    },
    "90a6c32ca2e847f5a18bc52eec6eecb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "90f55c34e39542598a0cb431e583e231": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "95352cc217234c619ef1e81de614a060": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9865fe6631184f1c8013a6b9e4f52900": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9daf4717e63c4b14aa394c0ecccaa2eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1b87efc01af4a34988202d1d3b6e8ec",
      "placeholder": "​",
      "style": "IPY_MODEL_3f45e20be2f04ecabbc73622354a5a82",
      "value": "tokenizer_config.json: "
     }
    },
    "9fcc52e487a646d9b9a9164c083dc48d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a26fc236d5be43f58b6da95ef1105c96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac71cf80e5c64c5989c346d2a55f0edf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1b87efc01af4a34988202d1d3b6e8ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2b909dde10943c69a3a499979d3aa0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d0a484bbf984c2a9951425980c540e1",
      "placeholder": "​",
      "style": "IPY_MODEL_b470198a05614eb3a6e2d50576f5c251",
      "value": " 2.20G/2.20G [00:05&lt;00:00, 265MB/s]"
     }
    },
    "b470198a05614eb3a6e2d50576f5c251": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c1dcf6aada6e4f0d992e4eb52636d7b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c304ca79d507445ebb3672d9cb6b93e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c95c3a4082374455914913a7ab3b4eb4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d01ec07fb11b4111b1b4b49d8de5529a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac71cf80e5c64c5989c346d2a55f0edf",
      "max": 608,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ec101e06da5b425bb28e87e73e31831e",
      "value": 608
     }
    },
    "d10cbd32766c44ba944529a02e800bfa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d23a34815bc9469096a67a7bcd3d0e89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dc2dc2b8107b41a38f375a3b25d787fa",
       "IPY_MODEL_8d9578820e1945babfa8b26aa28c63e9",
       "IPY_MODEL_4f0d6a36bc164c70ac06f4ff86cf3d4b"
      ],
      "layout": "IPY_MODEL_82d79d391012496680f31d8b8457b17e"
     }
    },
    "d58ea4749733430f9904abc364349c0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d607ec55faad4490a3d8cb3c96114c2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8a127df43714731946adb2a4e984694": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d8a2353ce1444af7875fb619835d2ebc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "da8da82d8f0448fe943f4660c21d0155": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc2dc2b8107b41a38f375a3b25d787fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95352cc217234c619ef1e81de614a060",
      "placeholder": "​",
      "style": "IPY_MODEL_a26fc236d5be43f58b6da95ef1105c96",
      "value": "tokenizer.model: 100%"
     }
    },
    "e157739a8517477c9bb458500f3168d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_78503531f0184b048229d9d5d7b95456",
       "IPY_MODEL_fb5a6eff497442409ac48a158c56f7ed",
       "IPY_MODEL_1f09a89a976a40f28cbb2e7276985310"
      ],
      "layout": "IPY_MODEL_1e7acf525eb847e78f0d60fa5dedfd3d"
     }
    },
    "e19d3a6f487047dba0e5ab92156ba06b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9daf4717e63c4b14aa394c0ecccaa2eb",
       "IPY_MODEL_1899b65d8b514ae8b4f1cdf171700f40",
       "IPY_MODEL_2156285d48a54ca591bb1ec0d90f0a8b"
      ],
      "layout": "IPY_MODEL_533c437b3bab43e592205c2a4d21b511"
     }
    },
    "e1a3126fd4e447eabfccca7ffdadd9f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8335a14d6b374b4db596695fdf8758e2",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_38765ed0c0c3474ebf17cd85ce8c1c83",
      "value": 1
     }
    },
    "e893006767644c3ebea56c476f6bd23b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec101e06da5b425bb28e87e73e31831e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f4fb66cc7cdb40efb19a91159249765a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_14d8ee36ad824b0cbf3d9d3510916cfa",
       "IPY_MODEL_3ebd73ab2cad48fca84013dd93d8dce0",
       "IPY_MODEL_b2b909dde10943c69a3a499979d3aa0f"
      ],
      "layout": "IPY_MODEL_87b2ac0b25ff40a7b23442f025338572"
     }
    },
    "f59a32c07c434b319b5df6979246dc8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb5a6eff497442409ac48a158c56f7ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c2c7bca63204ea0997e2276e9e6eab6",
      "max": 551,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9865fe6631184f1c8013a6b9e4f52900",
      "value": 551
     }
    },
    "fbd4d6247ee943929a490b9926cae60a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56f4d0f68528483bae5b5a3942098b33",
      "placeholder": "​",
      "style": "IPY_MODEL_c1dcf6aada6e4f0d992e4eb52636d7b4",
      "value": " 608/608 [00:00&lt;00:00, 87.4kB/s]"
     }
    },
    "fc8ba7e0f0344c2f84fc6c7335e81cf4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe2bc79a74fd48eb97fe148dd9d684b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe4012ce56354650b39387cbd23ef1e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe2bc79a74fd48eb97fe148dd9d684b0",
      "placeholder": "​",
      "style": "IPY_MODEL_2f6b119ba8db4c9989137b4e803b81f7",
      "value": "config.json: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
